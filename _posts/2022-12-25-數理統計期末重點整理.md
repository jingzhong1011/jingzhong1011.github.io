---
title: 數理統計期末重點整理
tags: 統計
---
## Preliminary: 常見的分佈

---

## Topic 4: Properties of a Random Sample

### Random Sample

#### General

$T(\underline{X})$：樣本的函數

$X_1\dots X_n \overset{\text{iid}}{\sim} f(x)$ then,

1.  $E(\bar{X})=\mu$
2. $Var(\bar{X})=\frac{\sigma}{n}$
3. $E(S^2)=\sigma^2$
4. $M_{\bar{X}}=[M_X (\frac{t}{n})]^n=\prod^n_{i=1}E[e^{\frac{t}{n}X_i}]$

#### Exponential family

(指數族的封閉性)

$X_1\dots X_n \overset{\text{iid}}{\sim} f(x;\theta)=h(x)c(\theta)\exp\{\sum^k_{j=1}w_j(\theta)t_j(x)\}$ 

the distribution of $T=(T_1,\dots ,T_k)$ $f_T(u_1,\dots,u_k)=H(u_1,\dots,u_k)[c(\theta)]^n\exp\{\sum^k_{j=1}w_j(\theta)u_j(x)\}$

#### Normal Distribution

1. $\bar{X}\perp S^2$：sample mean與sample variance獨立
2. $\bar{X}\sim N(\mu,\frac{\sigma^2}{n})$
3. $\frac{(n-1)S^2}{\sigma^2}\sim \chi^2_{n-1}$

#### T-Distribution & F-Distribution

Let $Z\sim N(0,1)\perp U\sim\chi^2_p$,

$T\triangleq \frac{Z}{\sqrt{\frac{U}{p}}}$

自由度$df=p$

$U\sim\chi^2_p\perp V\sim\chi^2_q$

$F\triangleq\frac{U/p}{V/q}\sim F_{p,q}$

自由度$df=(p,q)$

1. $X\sim F_{p,q}\Rightarrow\frac{1}{X}\sim F_{q,p}$
2. $X\sim t_q\Rightarrow X^2\sim F_{1,q}$
3. $X\sim F_{p,q}\Rightarrow\frac{\frac{p}{q}X}{1+\frac{p}{q}X}\sim Beta(\frac{p}{2},\frac{q}{2})$

### Order Statistic

$\{X_i\}^n_{i=1}\overset{\text{iid}}{\sim} f(x)\Rightarrow X_{(1)}<X_{(2)}<\dots <X_{(n)}$

#### cdf

$F_{X_{(j)}}(x)=\sum^n_{k=j}{n\choose k}[F(x)]^k[1-F(x)]^{n-k}$

#### pdf

1. Discrete $a_1<a_2<a_3<\dots<a_m$
$f_X(j)(a_m)=F_{X_{(j)}}(a_m)-F_{X_{(j)}}(a_{m-1})$ 
2. Continuous
    
    $f_{X_{(j)}}(x)=\frac{n!}{(j-1)!(n-j)!}f_{(X)}[F(X)]^{j-1}[1-F(X)]^{n-j}$
    
3. joint pdf
    
    $f_{X_{(i)},X_{(j)}}(u,v)=\frac{n!}{(i-1)!(j-i-1)!(n-j)!}f(u)f(v)[f(u)]^{i-1}[F(v)-F(u)]^{j-i-1}[1-F(v)]^{n-j}$
    
    then, $f_{X_{(1)}\dots X_{(n)}}(x_1\dots x_n)=n!\prod^n_{i=1}f(x_i)\cdot1(x_1<\dots x_n)$
    

### Convergence Concept

#### Jensen’s Inequality

iff $P(g(X)=a+bX)=1$, for any r.v. $X$ and any convex(concave) $g(\cdot)$, $E(g(X))\geq g(E(X))$

g是線性的才成立

#### 機率的收斂

A sequence of r.v. $Y_1\dots Y_n$ converges in probability to a r.v. $Y$ if $\forall \epsilon>0$, $\lim_{n\rightarrow\infty}P(|Y_n-Y|<\epsilon)=1$ denoted by $Y_n\overset{p}{\rightarrow}Y$

#### Law of Large Number

$X_1\dots X_n \overset{iid}{\sim}f(x), E(X)=\mu, Var(X)=\sigma^2$

Define $\bar{X}_n=\frac{1}{n}\sum^n_{i=1}X_i$

Then, $\bar{X}_n\overset{p}{\rightarrow}\mu$ㄈ

#### 分佈的收斂

A sequence of r.v. $Y_1\dots Y_n$ converges in distribution to $Y$

if $\lim_{h\rightarrow\infty}F_{Y_n}=F_Y(y)\forall y$ denoted by $Y_n\overset{d}{\rightarrow}Y$

#### Central Limit Theorem

$X_1\dots X_n \overset{iid}{\sim}f(x), E(X)=\mu, Var(X)=\sigma^2$

then, $\frac{\sqrt{n}(\bar{X}_n-\mu)}{\sigma}\overset{d}{\rightarrow}N(0,1)$

i.e. $\bar{X}_n\overset{a.}{\sim}N(\mu,\frac{\sigma^2}{n})$ for large $n$

#### Convergence的代換

1. $X_n\overset{p}{\rightarrow}X\Rightarrow X_n\overset{d}{\rightarrow}X$
2. $X_n\overset{d}{\rightarrow}c\Rightarrow X_n\overset{p}{\rightarrow}c$
3. if $X_n\overset{d}{\rightarrow}X, Y_n\overset{p}{\rightarrow}a$
    1. $Y_nX_n\overset{d}{\rightarrow}aX$ (By Slutsky Thm.)
    2. $Y_n+X_n\overset{d}{\rightarrow}a+X$ (By Slutsky Thm.)
    3. $g(Y_n)\overset{p}{\rightarrow}g(a)$  for conti. $g$ (By Continuous Mapping Thm.)
    4. $g(Y_n)\overset{d}{\rightarrow}g(X)$  for conti. $g$ (By Continuous Mapping Thm.)

#### Delta Method

Suppose $\sqrt{n}(Y_n-\theta)\overset{d}{\rightarrow}N(\theta,\sigma^2)$

for any $g(\cdot)$ s.t. $g'(\theta)\neq0$, we have $\sqrt{n}(g(Y_n)-g(\theta))\overset{d}{\rightarrow}[g'(\theta)]\cdot N(0,\sigma^2)$

---

## Topic 5: Principles of Data Reduction

#### Sufficient Statistic

$T(\underline{X})$ is a SS for $\theta$ if the conditional distribution of $\underline{X}$ given $T(\underline{X})$ is independent of $\theta$

Let $\underline{X}\sim P(\underline{x}|\theta),T(\underline{X})\sim q(t|\theta)$

Then, $T(\underline{X})$ is a SS for $\theta$ if $\frac{P(\underline{x}|\theta)}{q(t|\theta)}\perp\theta$

#### Factorization Theorem

Let $\underline{X}\sim f_{\underline{X}}(x|\theta)$,

$T(\underline{X})$ is a SS for $\theta$ iff $f_{\underline{X}}(x|\theta)=g(T(\underline{x});\theta)\cdot h(x)$

$X_1\dots X_n\overset{iid}{\sim}f(x|\theta)=h(x)\cdot c(\theta)\cdot \exp(\sum^k_{j=1}t_j(x)\cdot w_j(t))$

then, $T(\underline{X})=(\sum^n_{i=1}t_1(X_i),\dots,\sum^n_{i=1}t_k(X_i))$ is a SS for $\theta$

$X_1\dots X_n\overset{iid}{\sim}f(x|\theta)$ and $T(\underline{X})$ is a SS for $\theta$

then, $g(T(\underline{X}))$ is also a SS for $\theta$ for any monotone $g(\cdot)$

#### Minimum Sufficient Statistic

$T(\underline{X})$ is a M-SS if for any other SS $T^*(\underline{X})$, $T(\underline{X})$ is a function of $T^*(\underline{X})$

$f_{\underline{X}|T(\underline{X})}\perp\theta$

$\dim(T(\underline{X}))$ is minimal

#### Ancillary Statistic

$A(\underline{X})$ is an AS if $f_{A(\underline{X})}(a)\perp\theta$

與參數不相關

1. $T(\underline{X})$：SS for $\theta$, $E[T(\underline{X})]=\theta$
    
    $A(\underline{X})$：AS, $E[A(\underline{X})]\perp\theta$
    
    $W(\underline{X})\triangleq A(\underline{X})-E[A(\underline{X})]\perp\theta$ with $E[T^*(\underline{X})]=\theta$
    
2. $T^*(\underline{X})\triangleq T(\underline{X})+a\cdot W(\underline{X})$ for some $a$
    
    $Var(T^*(\underline{X}))=Var(T(X))+2a\cdot Cov(T(X), W(\underline{X})) + a^2 Var(W|\underline{X})$
    
    $\overset{T\perp A}{=}Var(T(\underline{X}))+a^2Var(W(\underline{X}))\geq Var(T(\underline{X}))$
    

#### Complete Statistic

Let $F_T(t|\theta)$ be a family of pdf/pmf of a statistic $T(\underline{X})$

Then, $\{f_T(t|\theta):\theta\in\Theta\}$ is called complete if $E_\theta [g(T)]=0\ \forall\theta$ implies $P_\theta (g(T)=0)=1$

Then, $T(\underline{X})$ is the C-S

Under$\{f_T(t|\theta):\theta\in\Theta\}$, the only 0 unbiased estimator is 0 itself

#### Basu’s Theorem

If $T(\underline{X})$ is a C-SS, the $T(\underline{X})$ is independent of any AS

$X_1\dots X_n\overset{iid}{\sim}f(x|\theta)=h(x)\cdot c(\theta)\cdot \exp(\sum^k_{j=1}t_j(x)\cdot w_j(t))$ with$\theta=(\theta_1\dots\theta_k)$

Then, $T(\underline{X})=(\sum^n_{j=1}t_1(x_i),\dots,\sum^n_{j=1}t_k(x_i))$ is a C-SS

---

## Topic 6: Point Estimation

### Method of Moment

$X_1\dots X_n\overset{iid}{\sim}f(x|\theta), \theta=(\theta_1\dots\theta_k)$

$\Rightarrow E[X^r]=\int x^r\cdot f(x|\theta)dx\triangleq\mu_r(\theta)$

$\Rightarrow\frac{1}{n}\sum^n_{j=1}x_i^r=\mu_r(\hat{\theta}),r=1,2,\dots,k$

$\Rightarrow \tilde{\theta}$ is an. estimator of $\theta$ from Method of Moment

### Maximum Likelihood Estimate (MLE)

$X_1\dots X_n\overset{iid}{\sim}f(x|\theta)$

MLE must be a function of M-SS

#### Likelihood function

Let $\underline{X}\sim f_{\underline{X}}(\underline{X};\theta)\overset{if\ iid}{=}\prod^n_{i=1}f(x_i;\theta)$

then, given $\underline{X}$, the function of $\theta$ denoted by $L(\theta)\triangleq f_{\underline{X}}(\underline{X};\theta)$

$\hat{\theta}\triangleq \argmax_\theta L(\theta)$ is called MLE of $\theta$

$=\argmax_\theta l(\theta)=ln L(\theta)$

在$\theta$下看到$\underline{X}$的強度

#### $\ln$-variance Property of MLE

If $\hat{\theta}$ is the MLE of $\theta$, then, for any function $\tau(\theta)$, its MLE is $\hat{\tau(\theta)}=\tau(\hat{\theta})$

### Method of Evaluating Estimates

#### Mean Square Error (MSE)

The MSE of $W(\underline{X})$ for $\theta$ is $E_\theta[(W(\underline{X})-\theta)^2]$

$MSE(w,\theta)=Var_\theta(w)+[Bias(w,\theta)]^2$ with $Bias(w,\theta)=E_\theta[w-\theta]$

An estimator $T(\underline{X})$ is called unbiased estimator if $Bias(T,\theta)=0$

#### UMVUE

An estimator $W^*$ is the best unbiased estimator of $\tau(\theta)$ if $E_\theta(W^*)=\tau(\theta)\ \forall\theta$ and for any $W$ s.t. $E_\theta(W)=\tau(\theta)\ \forall\theta$,

we have $Var_\theta(W^*)\leq Var_\theta(W)\ \forall\theta$

then $W^*$ is called the uniformly minimum variance unbiased estimator (UMVUE) of $\tau(\theta)$

#### Cramer-Rao Lower Bound (CRLB)

$\underline{X}\sim f_{\underline{X}}(\underline{X};\theta)$ and let $W(\underline{X})$ be an estimator satisfying $E[W(\underline{X})]=\tau(\theta)$

$\frac{\partial}{\partial\theta}E[W(\underline{X})]=\int \frac{\partial}{\partial\theta} W(\underline{x})\cdot f_{\underline{x}}(\underline{x};\theta)dx$

Then, $Var_\theta(W|\underline{X})\geq \frac{[\frac{\partial}{\partial\theta}\tau(\theta)]^2}{E_\theta[(\frac{\partial}{\partial\theta}\ln f_{\underline{X}}(\underline{x};\theta))^2]}$

#### Attainment of CRLB

$W(\underline{X})$ attains CRLB iff $a(\theta)[W(\underline{X})-\tau(\theta)]=\frac{\partial}{\partial\theta}\ln f_{\underline{X}}(\underline{X};\theta)$ for some $a(\theta)$

#### Rao-Blackwell Theorem

Let $W$ be any unbiased estimator of $\tau(\theta)$ and let $T$ be a SS for $\theta$

Define $\varphi(T)=E[W|T]$

Then, $E[\varphi(T)]=\tau(\theta),Var(\varphi(T))\leq Var(W)$

(UMVUE must be a function of M-SS)

#### Theorems of UMVUE

1. UMVUE is unique 唯一性
2. $W$ is UMVUE iff $Cov(W,U)=0\ \forall U$ s.t. $E[U]=0$

#### Lehmann-Schette Theorem

Let $T$ be a C-SS for $\tau(\theta)$ and $\varphi(T)$ is an estimator based on $T$ s.t. $E[\varphi(T)]=\tau(\theta)$,

then, $\varphi(T)$ is UMVUE of $\tau(\theta)$

Given any unbiased estimator $h(\underline{X})$ for $\tau(\theta)$,

$\varphi(T)\triangleq E[h(\underline{X})|T]$ is UMVUE of $\tau(\theta)$

|  | MM | MLE | UMVUE |
| --- | --- | --- | --- |
| unbiased | X | X | V |
| function of M-SS | X | V | V |
| optimality | X | Attain CRLB as $n\rightarrow\infty$ | V |
| non-parametric | V | X | X |
| implementation | V | V | X (不見得存在) |
| consistency | V | V as $n\rightarrow \infty$ | V |

---

## Topic 7: Hypothesis Testing and Interval Estimation

### Hypothesis Testing

#### Complementary Hypothesis

$H_0$ versus $H_1$

A test statistic $W(\underline{X})$ s.t. $H_0$ is rejected if $W(\underline{X})\in R$ where $R$ is the reject region

### Method of Finding Test

#### Likelihood Ratio Test (LRT)

Consider a simple hypothesis $\begin{array}{l}H_0:\theta=\theta_0\\H_1:\theta\neq\theta_0 \end{array}$

$X_1,X_2\dots X_n\overset{iid}{\sim}f(X;\theta)$

$L(\theta)\triangleq\prod^n_{i=1}f(X_i;\theta)$ tends to reject $H_0$ if $L(\theta_0)<L(\theta_1)$

$\begin{array}{l}H_0:\theta=\theta_0\\H_1:\theta\neq\theta_0 \end{array}$tends reject $H_0$ if $\sup_{\theta\in\Theta_0}L(\theta)<\sup_{\theta\in\Theta_1}L(\theta)$

then, tends to reject $H_0$ if $\sup_{\theta\in\Theta_0}L(\theta)<\sup_{\theta\in\Theta}L(\theta)$

比較0和全部

$\lambda(\underline{X})\triangleq\frac{\sup_{\theta\in\Theta_0}L(\theta)}{\sup_{\theta\in\Theta}L(\theta)}$

An LRT is the test with test function $\varphi(\underline{X})=1(\lambda(\underline{X})<c)$ for some $c\in(0,1)$

1. $\lambda(\underline{X})=\frac{L(\hat{\theta}_0)}{L(\hat{\theta})}$ (MLE of $\theta$ under $H_0$ / MLE of $\theta$)
2. for simple hypothesis$\begin{array}{l}H_0:\theta=\theta_0\\H_1:\theta=\theta_1 \end{array}$, $\lambda(\underline{X})\triangleq\frac{L({\theta}_0)}{L({\theta_1})}$

### Method of Evaluating $\varphi(\underline{X})$

透過Type I Error及Type II Error來評估$\varphi(\underline{X})$

|  |  | Decision |  |
| --- | --- | --- | --- |
|  |  | $H_0$ | $H_1$ |
| Truth | $H_0$ | Correct Decision | Type-I Error: $\alpha$ |
|  | $H_1$ | Type-II Error: $\beta$ | Power: $1-\beta$ |

#### Power Function of $\varphi(\underline{X})$

$\beta(\theta)\triangleq P_\theta(\lambda(\underline{X})<c)$

1. Type-II Error if $\theta\in\Theta_0$
2. 1-(Type-II Error) if $\theta\in\Theta_1$

$\varphi(\underline{X})\triangleq1(\lambda(\underline{X})<c)$

不可能做到optimal，但能盡量碰到其中一邊

控制Type-I Error，建立c使得Type-I Error是受到控制

$\Rightarrow\beta_c(\theta)=\alpha$, where $\alpha$ is user-defined

Determine $c$ s.t.

$\begin{array}{l}\sup_{\theta\in\Theta_0}\beta_c(\theta)=\alpha\\\sup_{\theta\in\Theta_0}\beta_c(\theta)\leq\alpha\end{array}$side $\alpha$ test / level $\alpha$ test

$\alpha\approx P(\lambda(\underline{X})<c|H_0)$

到這裡test建構完成

看想要保護哪個statement(控制犯錯的error)

#### UMP $\alpha$ Test

Let $C$ be a class of test for $\begin{array}{l}H_0:\theta\in\Theta_0\\H_1:\theta\in\Theta_1 \end{array}$

A test in $C$ with power function $\beta(\theta)$ is a uniformly powerful (UMP) class $C$ test if $\beta(\theta)\geq\beta'(\theta)\ \forall\Theta_1$, where $\beta'(\theta)$ is the power function of any other test in $C$

#### Neyman-Pearson Lemma

Consider LRT with $\varphi(\underline{X})=1(\lambda(\underline{X})<c)$ where $c$ is determined such that $\beta(\theta_0)=\alpha$

then, LRT is the UMP level $\alpha$ test (size $\alpha$ test)

#### p-value

Reject $H_0$ if $W>c_\alpha \Leftarrow 1(W>c_\alpha)$

$\Rightarrow\varphi(\underline{X})=1(g(W)>\alpha)$

p-value: $\alpha=P(W>c_\alpha|H_0)=1-F_W(c_\alpha)\triangleq S_W(c_\alpha)$

$\Rightarrow c_\alpha=S^{-1}_W(\alpha)$

$\Rightarrow g(\cdot)=S_W(\cdot)$

$\Rightarrow \varphi(\underline{X})=1(W>C_\alpha)=1(S_W(W)<S_W(C_\alpha))=1(S_W(W)<\alpha)$

$S_W(W)\triangleq p-value$

### Interval Estimation

1. For an interval estimator $[L(\underline{X}),U(\underline{X})]$, the coverage probability is $P_\theta(\theta\in[L(\underline{X}),U(\underline{X})])$
2. The confidence coefficient of $[L(\underline{X}),U(\underline{X})]$ is $\inf_\theta P(\theta\in[L(\underline{X}),U(\underline{X})])$

An $(1-\alpha)$ confidence interval (C.I.) has at least $(1-\alpha)$ coverage probability

#### Method of Finding Interval Estimation

For each $\theta_0\in\Theta$, let $A(\theta_0)$ be the acceptance region of a level $\alpha$ test with $H_0:\theta=\theta_0$.

For each sample $\underline{X}$, define $c(\underline{x})=\{\theta:\underline{x}\in A(\theta)\}$

then $c(\underline{x})$ is an $(1-\alpha)$ C.I. for $\theta$

得到A(參數的函數)之後，在固定的樣本$\underline{x}$之下，所有$\theta$滿足$\underline{x}\in A(\theta)$

固定資料，得到參數

 $c(\underline{x})$：參數的set

$\begin{array}{l}H_0:\theta>\theta_0\\H_1:\theta\neq\theta_0 \rightarrow [L(\underline{X}),U(\underline{X})]\\H_1:\theta<\theta_0  \rightarrow [L(\underline{X}),\infty)]\ or\ [-\infty,U(\underline{X})]\end{array}$

$H_1$的樣子會影響到$A(\theta_0)$

---

## Topic 8: Asymptotic Evaluation

A sequence of estimators $W_n=W(X_1\dots X_n)$ is a consitent estimator of $\theta$ if $W_n\overset{p}{\rightarrow}\theta$

#### Consistency of MLE

Let $X_1\dots X_n\overset{iid}{\sim}f(x;\theta_0)$

Let $\hat{\theta}$ be the MLE of $\theta_0$, then, $\tau(\hat{\theta})$ is a consistent estimator of $\tau(\theta_0)$

For any estimator $W_n$, suppose that $\sqrt{n}(W_n-\tau(\theta))\overset{d}{\rightarrow}N(0,\sigma^2)$

Then, $\sigma^2$ is called the asymptotic variance $W_n$

估計式的好壞，比較asymptotic variance，愈小者愈好

$W_n$ is asymptotically efficient if $\sqrt{n}(W_n-\tau(\theta))\overset{d}{\rightarrow}N(0,V(\theta))$ where $V(\theta)=\frac{[\frac{\partial}{\partial\theta}\tau(\theta)]^2}{E_\theta[(\frac{\partial}{\partial\theta}\ln f(X;\theta))^2]}$

(i.e. $W_n\sim N(\tau(\theta),\frac{1}{n}V(\theta))$ for large $n$)

#### Asymptotic Efficiency of MLE

Let $X_1\dots X_n\overset{iid}{\sim}f(X;\theta_0)$ and let $\hat{\theta}$ be the MLE of $\theta_0$

For any conti. $\tau(\theta)$, $\sqrt{n}(\tau(\hat{\theta})-\tau(\theta_0))\overset{d}{\rightarrow}N(0,V(\theta_0))$

#### Estimation of $V(\theta_0)$

$V(\theta)=\frac{[\frac{\partial}{\partial\theta}\tau(\theta)]^2}{I(\theta)}$

$I(\theta)\triangleq E[(\frac{\partial}{\partial\theta}\ln f(X;\theta))^2]=\frac{1}{n}\sum^n_{i=1}[\frac{\partial}{\partial\theta}\ln f(X;\theta)]^2$

$\sqrt{n}(\hat{\theta}-\theta_0)\approx[\frac{-1}{n}\frac{\partial^2}{\partial\theta^2}l(\theta_0)]^{-1}\frac{\sqrt{n}}{n}\frac{\partial}{\partial\theta}l(\theta_0)$

$[\frac{-1}{n}\frac{\partial^2}{\partial\theta^2}l(\theta_0)]^{-1}=[\frac{1}{n}\sum^n_{i=1}\frac{-\partial^2}{\partial\theta^2}\ln f(X_i;\hat{\theta})]^{-1}\overset{p}{\rightarrow}[T(\theta_0)]^{-1}$

$\frac{\sqrt{n}}{n}\frac{\partial}{\partial\theta}l(\theta_0)=\frac{1}{n}\sum^n_{i=1}(\frac{\partial}{\partial\theta}\ln f(X_i;\hat{\theta}))^2\overset{d}{\rightarrow}N(0,I(\theta_0))$

$\Rightarrow [\frac{1}{n}\sum^n_{i=1}\frac{-\partial^2}{\partial\theta^2}\ln f(X_i;\hat{\theta})]^{-1}\times[\frac{1}{n}\sum^n_{i=1}(\frac{\partial}{\partial\theta}\ln f(X_i;\hat{\theta}))^2]\times[\frac{1}{n}\sum^n_{i=1}\frac{-\partial}{\partial\theta^2}\ln f(X_i;\hat{\theta})]^{-1}$

is the **Sandwich-Type Estimator**

MLE: $\sqrt{n}(\hat{\theta}-\theta_0)\overset{d}{\rightarrow}N(0,V(\theta_0))$

$\Rightarrow P(|\frac{\sqrt{n}(\hat{\theta}-\theta_0)}{\sqrt{V(\theta_0)}}|\leq z_{\frac{\alpha}{2}})\rightarrow 1-\alpha$

$\Rightarrow P(\theta_0\in[\hat{\theta}-\sqrt{\frac{\hat{V}(\hat{\theta}_0)}{n}}\cdot z_{\frac{\alpha}{2}},\theta_0\in[\hat{\theta}+\sqrt{\frac{\hat{V}(\hat{\theta}_0)}{n}}\cdot z_{\frac{\alpha}{2}}])\approx1-\alpha$